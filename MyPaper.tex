% -*- Mode: LaTeX; Mode: visual-line; -*-
\documentclass[E]{compsoft}


\taikai{2019}
\pagestyle {empty}

\usepackage [dvipdfmx] {graphicx}

% ユーザが定義したマクロなどはここに置く．ただし学会誌のスタイルの
% 再定義は原則として避けること．

\begin{document}

\title{ASTToken2Vec: an Embedding Method for Neural Code Completion}

\author{Dongfang Li  \quad   Hidehiko Masuhara
%
% ここにタイトル英訳 (英文の場合は和訳) を書く．
%
\ejtitle{ASTToken2Vec: コード補完神経回路網のための記号埋め込みの一手法}
%
% ここに著者英文表記 (英文の場合は和文表記) および
% 所属 (和文および英文) を書く．
% 複数著者の所属はまとめてよい．
%
\shozoku{李 東方, 増原 英彦}{東京工業大学数理計算科学系}%
{Department of Mathematical and Computing Science, Tokyo Institute of Technology}}




\Eabstract{
Code completion systems help the programmers to write code more efficiently and to reduce typographical errors by automatically suggesting code fragments that are most likely to be written next. This work attempts to improve a neural network-based code completion system by proposing a new embedding method called ASTToken2Vec for program tokens. ASTToken2Vec is inspired by Word2Vec, and gives a vector representation to a program token so that two similar tokens will have representations closed to each other in the vector space.  We integrated ASTToken2Vec with a neural network-based code completion model and evaluated its prediction performance by using a dataset consisting of 150,000 open-source JavaScript program files.}
\maketitle \thispagestyle {empty}







\section{Introduction}
% what is code completion system, why it is useful.
Code completion is a feature of programming editors that suggests code fragments that are likely to be typed in by the programmer following to the program text just before the cursor position.  It helps the programmer to write code more quickly, to remember rarely used API names, and to reduce typographical errors.

% Define prediction context
In general, code completion systems make suggestions (we hereafter call \emph{predictions}) based on the program text before the cursor position (we hereafter call a \emph{context}).  Predictions are usually a list of identifiers (e.g., function, method and variable names), but some systems predict a sequence of tokens or a few lines of statements.

% problems in traditional code completion
Contexts determine the prediction performance.  For example, the standard completion feature in Eclipse JDT merely the type of the receiver expression as the context when it predict a method name of a method invocation expression.  Eclipse code recommender \cite{heinemann2011suite} additionally uses a sequence of method calls preceding to the cursor line as the context.  It can improve prediction 
since many libraries require client programs to call several functions/methods in specific orders.

Traditional code completion systems make limited use of context information.  For example, Eclipse code recommender, while it uses preceding method calls, ignores names of local variables and control flow around the cursor position.  %It also ignores of calls of methods defined in the current project as it relies on the knowledge of the usages of well known libraries.

% First, they 
%  make limited use of contexts.  As mentioned above, they tend to use 
% Most of them are based on statistical methods and suggest code only by simple term frequency which often relatively have a higher error rate. 
% Another problem is that they rely on static types (like Eclipse for Java) to filter out candidates. 
% However, the dependency on typing information limits their applicability to widely used in dynamically typed languages like Python.etc.

% deep learning models, the reason we can apply dl model to code completion
Deep learning-based code completion, which is actively studied in the last few years \cite{liu2016neural,dataset,raychev2014code,white2015toward}, takes a different approach to exploit context information as well as knowledge of the programming language and APIs.  
It uses a neural network model, more specifically, a recurrent deep neural network (RNN) that takes a sequence of program tokens and predicts the next token.  It can be understood as an application of the RNN-based sentence generation for natural languages.%RNNs are known to be successful in the domain of natural language texts. 

When applying an RNN to programming languages, structural information in program texts matters to prediction, while typical RNN-based systems for natural languages  treat a text merely as a sequence of words.  There are several attempts to incorporate structural information in programs into RNN-based neural networks \cite{bielik2016phog,liu2016neural,raychev2016probabilistic}.  They basically encode an abstract syntax tree into a sequence of tokens with flags so that the sequence will have sufficient information to reconstruct the tree structure.

\begin{figure*}[!ht]
\centering
\includegraphics[scale=0.9]{pictures/overview.pdf}
\caption{Overview of ASTToken2Vec LSTM integration model}
\label{fig:overivew_model}
\end{figure*}
% introduce our work: node2vec 
% The embedding method is a way to represent discrete variables as continuous vectors. 
This paper proposes an embedding method called ASTToken2Vec for abstract syntax tree (ASTs) nodes of a program in order to improve the prediction performance of the deep learning-based code completion. 
It is a four-layer neural network that takes one AST token along with a few contextual tokens as input, and outputs one vector that represents the AST token.  The idea is based on the Word2Vec embedding \cite{word2vec}.  Similar to Word2Vec, the intuition behind the embedding is to give two semantically similar tokens vector representations that are close to each other.
As a code completion system, whose overview is shown in Figure~\ref{fig:overivew_model}, we use the embedding method as a frontend to an long short-term memory (LSTM)-based neural network that predicts next AST token from a sequence of AST tokens.  LSTM is a variant of RNN that explicitly manages long-term dependent information, and known to be useful to many recognition problems in natural languages.
% The vector representation will be used to learn and predict in a code completion system called the AT2V-LSTM model.  
% to learn and encode context information of AST tokens to generate semantic-based embedding representation vectors that contain more knowledge hidden behind ASTs. 
% We use this ASTToken2Vec model as a pre-trained model and integrate it with an LSTM based model as a code completion system to predict next tokens. 
% We name our integration model as AT2V-LSTM model and the overview of our model illustrates in Figure \ref{fig:overivew_model}.

As an experiment, we implemented both the ASTToken2Vec embedding and the AT2V-LSTM model, and trained them with a JavaScript AST dataset \cite{dataset}, which is a  collection of open-source programs containing a total of 150,000 JavaScript files. 
The validity of the embedding is manually confirmed by observing a set of example cases.  The experiment with the dataset showed slight improvements in accuracy with the ASTToken2Vec embedding compared to straightforward embedding.
% We visualize representation vectors of several terminal tokens and evaluate the performance of predicting next tokens by AT2V-LSTM integration model.
% From the results analysis, we conclude that the ASTToken2Vec method is able to generate semantic-based representation vectors of AST nodes and the AT2V-LSTM integrated model predicts tokens more accurately.




\section{Background}
% This is related work.  (issue #3)
Hindle et al.\  \cite{hindle2012naturalness} used the \textit{n-gram} technique for code completion.  Their work showed that programming code is more repetitive than natural language texts.
% this is also related work: TODO: read the paper
Nguyen et al.\  \cite{DBLP:journals/corr/MaddisonT14} propose generative models of natural source code with hierarchical structure and a distributed representation of source code element. 
They also leverage compiler logic and abstractions to improve their generative models.
% this is also related work.
Tung et al.\  \cite{Nguyen:2013:SSL:2491411.2491458} extends the state-of-the-art \textit{n-gram} approach which is called SLAMC by incorporating semantic information into code tokens. 

% related work, as it is statistical
Allamanis et al.\ proposed a statistical nonparametric Bayesian probabilistic tree-based system for extracting programming idioms from a source code text
\cite{allamanis2014mining}.
% related work, TODO cannot understand...
Liang et al.\  \cite{liang2010learning} focus on learning programs for multiple related tasks with a few training samples and propose a nonparametric hierarchical Bayesian model which is able to share the statistical information across multiple tasks for code completion.

% related work, TODO what is "by allowing conditioning of a production rule. "???
Bielik et al.\  \cite{bielik2016phog} proposed a generative model for code called the probabilistic higher-order grammar. This model can capture context information between tokens by allowing conditioning of a production rule. 
% this is ...? a subsequent work of raychev2014code, background???
Raychev et al.\  \cite{raychev2016probabilistic} create a domain-specific language (DSL) over abstract syntax trees (ASTs) called TGEN which can encode an AST to a specific language context.
They also propose a special decision tree called DEEP3 which can make code predictions leveraging the AST context encoded by the TGEN model.

% Raychev is RNN but we do not directly based on this!???
Raychev et al.\  \cite{raychev2014code} and White et al.\  \cite{white2015toward} explore how to apply RNN models on sequences of tokens to facilitate the task of code completion.
% This is the work we based on.  Hmm, it is a rejected paper.
Liu et al.\  \cite{liu2016neural} propose several LSTM-based models for code completion with an AST dataset and they leverage the ASTs by converting ASTs to sequences of training samples. 
Their work gives us inspiration about how to convert an AST to a sequence and how to train an LSTM model with AST dataset. 

% TODO: tree encoding
% TODO: Word2Vec


\section{Abstract Syntax Tree}
% 介绍AST的详细内容，结构等
An abstract syntax tree (AST) is a structure that represents structural information of programs. It is widely used in code completion.
%ASTToken2Vec model training and AT2V-LSTM integration model training.

% this is background information
An AST consists of two kinds of nodes, namely \emph{non-terminal nodes} and \emph{terminal nodes}.
A non-terminal node is a node that has one or more children nodes. 
For example, in JavaScript, non-terminal nodes correspond to syntax categories such as function declarations, variable declarations, \texttt{for} statements, \texttt{if} statements, and \texttt{while} statements.
% These non-terminal tokens declare what kind of functions or variables specified in the program. 
% Other kinds of non-terminal tokens represent more knowledge about the structure and logical judgment of a program like ``ForStatement'', ``IfStatement'', ``WhileStatement'', etc. 

A terminal node is a node that appear only at leaf positions in an AST.
We call the lexical token of a terminal code as a \emph{value}.
In JavaScript, terminal nodes correspond to to syntax categories such as identifiers (variable, field and function names), literal numbers, and literal strings.  

% Due to ASTs contain more semantic knowledge about source code, we use ASTs as the basic data to generate two sets of training samples for our models' training.



\section{ASTToken2Vec Embedding}
\label{section:node2vec}
ASTToken2Vec is an embedding neural network model for AST nodes. It converts between a program token and a point in a vector space.  The latter representation can be used as an input/output of the neural network model for code completion.  

Intuitively, we design ASTToken2Vec to incorporate similarities of tokens into embedded representations, where two similar programs tokens will have two points in the vector space that are close to each other.  By similarity, we here mean similarity in natural language semantics (such as ``\texttt{size}'' and ``\texttt{length}'') as well as similarity in the API usage (such as \texttt{Stack.peek()} and \texttt{Stack.pop()}).

ASTToken2Vec is inspired by Word2Vec \cite{word2vec}, which gives vector representations to natural language words. Differently from Word2Vec, ASTToken2Vec 
uses two identical networks for embedding non-terminal and terminal tokens.  It also separately provides non-terminal and terminal tokens as context information for training.

% ASTToken2Vec trains a four-layers neural network to generate the semantic-based representation vectors of AST nodes which enable the LSTM-based code completion model to leverage more structural knowledge to predict next tokens.
% In order to do that, we give a basic hypothesis of ASTToken2Vec which is same as Word2Vec's: we assume that if two nodes in an AST have a similar context, the meaning of these two nodes also has a high-level similarity. 
% We specified the surrounding non-terminal nodes of a target node as the non-terminal context and surrounding terminal nodes as the terminal context. The details of contexts is explained in subsection\ref{sub:embedding_nt} and subsection\ref{sub:embedding_tt}.


\subsection{Model Architecture}
\begin{figure}[!ht]
\centering
\includegraphics[width=7.5cm, height=4cm]{pictures/node2vec_structure.png}
\caption{Architecture of NT2V (1) and TT2V (2)}
\label{fig:node2vec-structure}
\end{figure}
ASTToken2Vec is a pair of four layers neural networks, each contains one input layer, one hidden layer, and two output layers as shown in Figure~\ref{fig:node2vec-structure}.  Two identical, yet independent networks are for embedding non-terminal and terminal nodes, respectively.  We hereafter call the networks NT2V and TT2V, respectively.

The input layer uses the one-hot encoding representation, where $i$'Th token in a vocabulary of $V$ different tokens is represented as an $V$-dimensional vector all but $i$'Th element is zero.  
The two output layers represent the context of non-terminal and terminal nodes, each of which has the same vector size to the non-terminal/terminal input vectors.
We use the values in the hidden layer as the embedding vector of the input values.  The size of the hidden layer, a hyperparameter $D$, is determined by heuristics.
In the training phase, we calculate a joint loss function to update the model.

% Figure \ref{fig:node2vec-structure}(1) illustrates the architecture of the NT2V model and Figure \ref{fig:node2vec-structure}(2) is the architectrue of TT2V model.



\subsection{Embedding for Non-terminals (NT2V)}
\label{sub:embedding_nt}
The NT2V embedding uses both the terminal and non-terminal contexts for training.
A training tuple for NT2V contains three elements: (\textit{target non-terminal, non-terminal context, terminal context}) where the \textit{target non-terminal} is the non-terminal token which the NT2V model generates embedding vector for. 
The second and third elements are lists representing the contexts of the input \textit{target non-terminal}.
We define these two contexts as follows.

\subsubsection{Non-terminal context}
The non-terminal context for a non-terminal token means the surrounding non-terminal nodes in an AST. 
Concretely, we define the $n$ ancestor non-terminal nodes of a target non-terminal and its all non-terminal children nodes as its non-terminal context.
Here, $n$ is a hyperparameter. 
% If $n$ is relatively small, it means NT2V model does not consider the surrounding non-terminal tokens which are far from the target node as the non-terminal context.

\subsubsection{Terminal context} 
We define the terminal context of a non-terminal as all the direct children terminal nodes. 
% When there is no terminal children nodes (all its children nodes are non-terminal tokens or it does not have any children), we use a special terminal token: \textit{TT-EMPTY} to declare an empty terminal context for it.

We use an example of a partial AST to illustrate the contexts of a non-terminal in the Figure~\ref{fig:node2vec-context}. The boxes and ovals are non-terminal and terminal nodes, respectively.  
Let us focus the middle non-terminal node ``minv'' (method invocation).  The context of this node (where $n=2$) is illustrated as a shadowed area, consisting of the two ancestor nodes, namely ``if'' and ``block'', one non-terminal child ``fget'' (field get) and the three terminal nodes, namely \texttt{cancel}, \texttt{flag} and \texttt{timeOut}.
%Hyperparameter $n$ here is two.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.9]{pictures/ast-context.pdf}
\caption{An example of contexts.  Boxes and ovals are non-terminal and terminal nodes, respectively.  The nodes in the gray area are the context of the method invocation (minv) node in the middle.  The nodes surrounded by a dashed line are the context of the terminal node \texttt{flag}.}
\label{fig:node2vec-context}
\end{figure}





\subsection{Embedding for Terminals (NT2V)}
\label{sub:embedding_tt}
The ASTToken2Vec model for terminal tokens' embedding vectors generation is abbreviated as TT2V.
Similar with NT2V model, the structure of training tuples for TT2V is (\textit{target terminal, non-terminal context, terminal context}) 
NT2V model leverage both non-terminal context and terminal context to generate the embedding vectors for the \textit{target terminal}.
The non-terminal context and terminal context of a target terminal are described as below.

\subsubsection{Non-terminal context} 
Because there is no child node of a terminal node, we only consider the non-terminal context of a terminal as only its $n$ parent non-terminal nodes. 
Same with NT2V, hyperparameter $n$ is used to define the scope of non-terminal context.

\subsubsection{Terminal context} 
We define the terminal context of a target terminal node as $m$ neighbor terminal nodes in an AST. 
Neighbor terminal nodes of a target terminal mean its sibling terminals which have the same parent non-terminal with the target terminal.
Here $m$ is a hyperparameter to specify the size of the terminal context. 
A relatively small $m$ means TT2V model does not consider too many surrounding terminal nodes as the terminal context. 
If a terminal node does not have any neighbor terminal nodes which means its parent node only has one single terminal child, in this case, we use a special terminal node: \textit{TT-EMPTY} to represent an empty terminal context. 

The partial AST in the Figure~\ref{fig:node2vec-context} shows a concrete example of what is contexts for terminals.
We focus the target terminal node \texttt{flag}.   The context of the node, where $n=2$ and $m=1$, is the area surrounded by a dashed line in the figure, consisting of two non-terminal nodes ``if'' and ``minv'', and two terminal nodes \texttt{cancel} and \texttt{timeOut}.
% Non-terminal nodes: ``NT-2'' and ``NT-4'' which are emphasized by a rectangle, represent the non-terminal context and terminal nodes ``TT-1'' and ``TT-3'' are the terminal context of the target node. 
% In this example, hyperparameter $n$ is specified as two and $m$ is equal to one.

\subsection{Joint Loss Function}
Due to there are two output layers in our ASTToken2Vec model, we design a joint loss function combining the non-terminal context output and terminal context output. 

There are three parts of the loss function calculation. 
$\mathit{Loss}_{\mathit{NT}}$ is used to represent the loss of non-terminal context output. 
The loss of terminal context output is represented by $\mathit{Loss}_{\mathit{TT}}$. 
Both of them are multi-labels loss calculations because there are more than one surrounding tokens as the context. 
 $\mathit{Loss}_{\mathit{total}}$ is the final joint loss function for our model's training.

\begin{equation}
\mathit{Loss}_{\mathit{NT}} = -\sum_{i=1}^{N}(y_{\mathit{NTcontext}}^{i} \times \log(\hat{y}_{\mathit{NTcontext}}^{i}))\label{equ:lossnt}
\end{equation}

Equation~(\ref{equ:lossnt}) is the $\mathit{Loss}_{\mathit{NT}}$ calculation formula which is a logarithmic loss function. 
Concretely, for an input token $x$, ASTToken2Vec model calculates the non-terminal context output as $\hat{y}_{\mathit{NTcontext}}$ and $y_{\mathit{NTcontext}}^{i}$ represents its ground-truth non-terminal context.
$N$ is the size of the non-terminal vocabulary. 


\begin{equation}
\mathit{Loss}_{\mathit{TT}} = -\sum_{j=1}^{M} (y_{\mathit{TTcontext}}^{j} \times \log(\hat{y}_{\mathit{TTcontext}}^{j}))\label{equ:losstt}
\end{equation}

Equation~(\ref{equ:losstt}) illustrates the formula of $\mathit{Loss}_{\mathit{TT}}$ calculation which also a logarithmic loss function. 
$\hat{y}_{\mathit{TTcontext}}$ is the terminal output of our model. and $y_{\mathit{TTcontext}}$ is the ground truth label of the terminal context.
$M$ is the size of terminal vocabulary.

\begin{equation}
\mathit{Loss}_{\mathit{total}} = \alpha  \mathit{Loss}_{\mathit{NT}} + (1-\alpha)  \mathit{Loss}_{\mathit{TT}}\label{equ:totalloss}
\end{equation}

Equation~(\ref{equ:totalloss}) is the joint loss function combining $\mathit{Loss}_{\mathit{NT}}$ and $\mathit{Loss}_{\mathit{TT}}$. 
We utilize a hyperparameter $\alpha$ whose range is from zero to one to adjust the importance between the loss of non-terminal context output $\mathit{Loss}_{\mathit{NT}}$ and terminal context output $\mathit{Loss}_{\mathit{TT}}$.




\section{AT2V-LSTM Integration}
\label{section:n2v-lstm-integration}
We integrate a basic LSTM model with our ASTToken2Vec embedding method. This integration model is called AT2V-LSTM which is able to leverage the semantic-based information extracted by ASTToken2Vec embedding and predict the next tokens as code completion.


\subsection{Sequences of Training Samples}
In order to train the linear-structured LSTM model, we convert ASTs to sequences of training samples.
Basically, we first convert an AST to a left-child-right-sibling (LC-RS) binary tree. 
Then, we transform this LC-RS binary into a complete binary tree by padding a special non-terminal node \textit{NT-EMPTY}.
Next, we apply a deep-first in-order traversal on this complete binary tree to generate a visiting sequence of training samples. 
There are four elements in a sample: (\textit{non-terminal, terminal, node-or-leaf, right-or-left}).
Concretely, when a non-terminal node is visited, it is considered as a target \textit{non-terminal} and the first element in the sample.
The second element: \textit{terminal} is the children terminals of the target \textit{non-terminal}.
If the target \textit{non-terminal} does not have any terminal child, we use a specified terminal token \textit{TT-EMPTY} to represent its empty child.
The last two elements: \textit{node-or-leaf} is used to declare whether the \textit{non-terminal} is a leaf or not in the complete binary tree and \textit{right-or-left} represents the position relationship between the target \textit{non-terminal} and its parent node. 
These two elements are used to reconstruct the predicting AST from the sequence of training samples.



\subsection{Model Architecture}
The architecture of our integration model is illustrated in the Figure \ref{fig:nti2p_model_architecture}. 
It contains one input layer, one LSTM layer, and an output layer. 


\begin{figure}[!ht]
\centering
\includegraphics[height=4cm, width=7.5cm]{pictures/lstm_structure.png}
\caption{The architecture of AT2V-LSTM model}
\label{fig:nti2p_model_architecture}
\end{figure}


\subsubsection{Input layer}
The input layer is a combination layer of the representation vectors of feeding elements and these vectors are initialized by ASTToken2Vec embedding.
The input is a sequence of training samples and one sample has four elements: $(N_{i}, T_{i}, NL_{i}, RL_{i})$ where $N_{i}$ is a non-terminal, $T_{i}$ is a terminal, $NL_{i}$ is the type information of $N_{i}$ and $RL_{i}$ is the side information of $N_{i}$. All these four elements are encoded by one-hot encoding and multiply embedding matrices as below:

\begin{equation}
\mathit{Input}_{i} = \mathit{Concat}(A\cdot N_{i} + B\cdot T_{i}, \quad C\cdot NL_{i} + D\cdot RL_{i})\label{equ:input}
\end{equation}
where $A, B$ are the embedding matrices for non-terminals, terminals and initialized by embedding vectors generated by ASTToken2Vec. $A$ is a $K \times V_{N} $ matrix and the shape of matrix $B$ is $K \times V_{T}$. $K$ is the length of embedding vectors. 
$V_{N}$ and $V_{T}$ are the size of non-terminal vocabulary and terminal vocabulary respectively. 
$C, D$ are embedding matrices for type information, and side information.
% initialized randomly. 
%The shape of $C$ and $D$ is $J \times 2$ where $J$ is the length of embedding vector for $NL_{i}$ and $RL_{i}$.


\subsubsection{LSTM layer}
The LSTM layer receives the embedding vectors from the input layer as $x_{i}$ and takes the output $h_{t-1}$ and hidden state $c_{t-1}$ from the previous state of LSTM layer. 
Then, the LSTM layer computes three operating gates: forget gate, update gate and output gate to calculate a new hidden state as $h_{t}$ and the new output as $c_{t}$.



\subsubsection{Output layer}
The output layer has four trainable matrices as the linear mapping between the output of the LSTM layer and the prediction. 
There are four instances our model predicting: 
next non-terminal $N_{i+1}$, next terminal $T_{i+1}$, type and side information of the next non-terminal, $NL_{i+1}$ and $RL_{i+1}$. 
The formula of output layer is as below:

\begin{equation}
P_{i} = \mathit{softmax}(W \times h_{i} + b)
\end{equation}
where $P$ is the prediction of next training sample including $p_{n}, p_{t}, p_{nl}$ and $p_{rl}$ representing the prediction of next non-terminal, next terminal, the side and type information of $p_{n}$. 
$W$s are four trainable matrices for linear mapping and $h_{i}$ is the output of the LSTM layer.
The softmax function returns the possibility of the next tokens predicting.
 




\section{Experiments}
\label{section:experiment}
\subsection{Dataset}
The dataset we used for both ASTToken2Vec and AT2V-LSTM  is the same one in the previous work~\cite{dataset}.  It consists of 150,000 JavaScript ASTs, each of which corresponds to one open-source program file.   We used 100,000 and 50,000 of them for training and evaluation, respectively.

% 介绍nonterminal的数据集，并解释增加了两个新bit，扩展了nonterminal的种类
\subsubsection{Non-terminal Vocabulary} 
There are 44 different kinds of non-terminal tokens specified by the JavaScript programming language grammar. 
Base on these 44 non-terminals, we add two more bits of information: whether the non-terminal token has a child token; whether this non-terminal has a right sibling or not. 
These two bits care more about the surrounding context of non-terminals and make the task of non-terminal prediction more challenging. 
This adjunction is also used in the previous work \cite{liu2016neural,dataset}. 
In total, there are 98 different tokens in the vocabulary including the special token, namely \textit{NT-EMPTY}, for representing the end of children nodes.% we use as a padding token to build a complete binary tree from an AST.

\subsubsection{Terminal Vocabulary}
In order to deal with too many different terminal tokens, we keep top 50,000 most frequently occurred terminal tokens in the original ASTs while replacing the rest tokens with the special token \emph{UNK} (unknown).  
% Theoretically, there are infinite kinds of terminal tokens may be included in programs.
% So, we use the idea of \textit{Word of Bag} to specify the terminal vocabulary. 
% Concretely, we sort all terminal tokens appearing in the training dataset by their frequencies of occurrence. 
% Then we choose the 50,000 most frequent terminal tokens as the vocabulary of the terminal. 
% For infrequent terminal tokens (out of our terminal vocabulary bag), we use a special terminal token \textit{UNK} to represent these terminals. 
We have 50,002 tokens in the vocabulary after added another special token \textit{TT-EMPTY} for representing absence.

% NOTE: Bag of words representation is a technique to represent a
% multiset of words by summing up the 1-hot encoding word
% representations.

\subsection{Experiment of ASTToken2Vec}
\subsubsection{Training details}
We implement ASTToken2Vec models to generate embedding vectors for both non-terminals and terminals. We define the size of the hidden layer $D$ (i.e., the vector length of the embedded representation) as 1,000. 
We specify the parameter $\alpha$ in the joint loss function as 0.6.
We use the Adam optimization algorithm with the learning rate of 0.002 to train models. 
The size of the training batch for ASTToken2Vec is $b=100$ and the training epoch is $e=10$.

\subsubsection{Visualization}
We visualize the representation vectors of several terminal tokens to show the performance of ASTToken2Vec.
We first apply principal component analysis (PCA) algorithm to these vectors to reduce the dimension from 1000 to 2. 
Then we normalize the 2-d vectors with min-max normalization so that the entire range of values of elements is $-2$ to $2$.

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.34]{pictures/node2vec_visualization.png}
\caption{The visualization of embedding vectors}
\label{fig:node2vec_visualization}
\end{figure}

We examined distributions of several terminal tokens as shown in Figure~\ref{fig:node2vec_visualization} in order to see whether the embedding
represents intuitive similarity.  In the figure, the names of tokens are written on the projected position of its vector representation.  The clusters of tokens near \texttt{length} (around the bottom left corner) and near \texttt{name} (around top left) are identifiers.  The numbers are clustering around the bottom right.  The two clusters at the top (i.e., around \texttt{value} and \texttt{push}) are properties, which appearing as a field name of an object.  The cluster at the bottom right (i.e., near \texttt{mouseDown}) are string literals\footnote{In JavaScript, field \texttt{f} of object \texttt{o} can be accessed by \texttt{o.f} as well as \texttt{o['f']}, some string literals are as important as field names.}.
% Terminal token \textit{Identifiers} are blue, \textit{LiteralNumbers} are represented by purple, \textit{Property} terminals are green token and red tokens are \textit{LiteralString} in the figure.

We can make the following observations.
\begin{itemize}
\item Different kind of terminal tokens (e.g., identifiers and property names) make different clusters.  This would mean that the embedding reflects the usage of tokens.
\item Tokens that have similar meaning in the natural language are placed at the positions close to each other.  For example, \texttt{length}, \texttt{size}, \texttt{len} and \verb|_len| make one cluster.
\item Similar to Word2Vec, the embedded representation would be compositional.
  The positions of \texttt{mouseup}, \texttt{mousedown}, \texttt{keyup} and \texttt{keydown} suggest that their positions can be decomposed into the four vectors, namely ``mouse,'' ``key,'' ``up,'' and ``down.''
\end{itemize}
Though we here can only discuss based on intuitive similarity, we would conclude the result of the embedding is reasonable.

% In the figure, there are several clusters like: ``literal string cluster'', ``property cluster'' which represent different types of terminals. 
% We also find that even tokens are the same type, if their meaning is different, they are still not in the same cluster. 
% For example, even the type of ``append'' and ``value'' are \textit{Property}, they are still far from each other because ``append'' is a property which can add some elements to a container in the most cases.
% However ``value'' usually is a member in a class without some operation functionality of a container. 
% Another example is ``length'' and ``userName''.
% Due to they have a different meaning, they are not in the same cluster even they are all \textit{Identifiers} 

% calculate similarity
%We also calculate the cosine similarity between these embedding vectors and the result meets our conclusion of visualization. 
% The most similar token to ``identifier length'' is ``identifier size'' with the similarity 0.927 and the second similar token is ``identifier len'' with the similarity 0.883. 



\subsection{Experiment of AT2V-LSTM}
We next use the proposed embedding for code completion to see whether it can improve accuracy of prediction.

\subsubsection{LSTM Training}
We implemented and compared two code completion models, namely the baseline LSTM model and our AT2V-LSTM model.  The two models have the LSTM model yet use different representations of tokens.
We use Adam optimization algorithm to train our model with base learning rate 0.0025 and it multiplies 0.9 every epoch as learning rate decay. 
We clip the gradient which is more than 6 to 6 and less than $-6$ to $-6$ to avoid the gradient exploding problem.
We specify the time sequence $s=50$ and the batch size is $b=100$, therefore, there are $s \times b = 5000$ training instances for one training batch. 
We train two models $e=10$ epochs.

\subsubsection{Next Non-terminal Prediction} 
Valid accuracy curve of the next non-terminal token prediction during the training phase is illustrated in Figure \ref{fig:valid_accuracy_for_non_terminal}. 
The blue curve represents the validation accuracy of the basic LSTM model and the orange curve is our AT2V-LSTM integration model. 
%The x-axis is the validation checkpoint among training step, there are four checkpoints for each training epoch. Due to we train our models 10 epochs totally in the experiment, there are 40 checkpoints to record the validation accuracy. 

From the validation accuracy curves, we find the non-terminal prediction accuracy of AT2V-LSTM integration model is a little higher than the accuracy of the basic LSTM model. The evaluation result illustrated in Table~\ref{table:non-terminal-evalution-accuracy} also shows that the accuracy of the integration model is 1.5\%-point better than the baseline model. 

\begin{figure}[!ht]
\centering
\includegraphics[scale=0.25]{pictures/nt_valid_accuracy.png}
\caption{Validation accuracy for non-terminal prediction during the training phase}
\label{fig:valid_accuracy_for_non_terminal}
\end{figure}



\begin{table}
\centering
\begin{tabular}{ccc}
\hline
Models& Top 1 accuracy& Top 3 accuracy\\
\hline
Baseline LSTM& 83.5 $\pm$ 0.2\% & 92.6 $\pm$ 0.2\% \\
AT2V-LSTM& 85.2 $\pm$ 0.2\% & 94.4 $\pm$ 0.2\% \\
\hline
\end{tabular}
\caption{Non-terminal evaluation accuracy}
\label{table:non-terminal-evalution-accuracy}
\end{table}



\subsubsection{Next Terminal Prediction} 
Figure \ref{fig:valid_accuracy_for_terminal} illustrates the validation accuracy curve for next terminal token prediction during the training phase. 
Orange curve represents our AT2V-LSTM integration model and the blue curve is the accuracy of the baseline LSTM model. 
The evaluation accuracy for the terminal in the test phase is shown in Table\ref{table:terminal-evalution-accuracy}. 
From both the evaluation result, we can find that the AT2V-LSTM integration model has a better performance with the predicting accuracy of 78.9\% than the baseline model 75.8\%. 


\begin{figure}[!ht]
\centering
\includegraphics[scale=0.25]{pictures/tt_valid_accuracy.png}
\caption{Validation accuracy for terminal prediction during the training phase}
\label{fig:valid_accuracy_for_terminal}
\end{figure}


\begin{table}
\centering
\begin{tabular}{ccc}
\hline
Models& Top 1 accuracy& Top 3 accuracy\\
\hline
Baseline LSTM& 75.8 $\pm$ 0.2\% & 87.7 $\pm$ 0.2\% \\
AT2V-LSTM& 78.9 $\pm$ 0.2\% & 89.2 $\pm$ 0.2\% \\
\hline
\end{tabular}
\caption{Terminal evaluation accuracy}
\label{table:terminal-evalution-accuracy}
\end{table}


\subsubsection{Next Token Type Prediction}
The evaluation result of the type (non-leaf or leaf) and the side (right-child or left-child) is shown in  Table~\ref{table:node-information-evaluation-accuracy}.
We can find that both two models achieve a good performance that the accuracy of type information is near to 97\% and side information is near to 95\%. 


\begin{table}
\centering
\begin{tabular}{ccc}
\hline
Models& Type accuracy& Side accuracy\\
\hline
Baseline LSTM& 97.6 $\pm$ 0.2\% & 94.8 $\pm$ 0.2\% \\
AT2V-LSTM& 97.8 $\pm$ 0.2\% & 95.1 $\pm$ 0.2\% \\
\hline
\end{tabular}
\caption{Type and side evaluation accuracy}
\label{table:node-information-evaluation-accuracy}
\end{table}

\subsubsection{Manual comparison}
To understand the effect of embedding, we manually analyzed the cases where two models gave different answers.  Although this is not systematic comparison at all, some cases gave us potential strength of the ASTToken2Vec. %the possible reasons that may cause our integration model to work better for some evaluation cases. 

\begin{figure}[!ht]
\centering
\includegraphics[height=8cm, width=7.4cm]{pictures/code_snippets_compare.png}
\caption{A case where embedding gave different results}
\label{fig:code_snippets_compare}
\end{figure}

The code snippet in Figure~\ref{fig:code_snippets_compare} is a test case where two models predicted different tokens.  The hole, shown as a light blue box in the code, is the token to predict.  The answer here was \texttt{pageY}, which was correctly predicted by ASTV-LSTM but not by the baseline model, which answered ``UNK.''  

One possible explanations for the difference is that the number of examples that use \texttt{pageX} and \texttt{pageY}.  As we can intuitively imagine those two tokens are not as common as \texttt{length} or \texttt{key}.  This would mean, even if the training dataset has code with \texttt{pageX} and \texttt{pageY}, it probably does not have the code that uses them in the same way (i.e., as a pair of field accesses in an array construction).  Therefore, it would be reasonable for the baseline model to answer ``UNK.'' 

With ASTToken2Vec, it would be possible to learn that \texttt{pageX} and \texttt{pageY} appear as field names of an event.  Then they would probably have vector representations that are close to other field names of events.  Then, if there were code fragments in the training dataset that construct an array of two field values of an event, that knowledge can be applied to \texttt{pageX} and \texttt{pageY} thanks to similarity.

%  what token should be filled in the hole.
% The expecting token is ``Property pageY''.
% The basic model predicts terminal ``UNK'' which means this model consider the token appearing in the hole is a quite uncommon terminal.
% However, our AT2V-LSTM model gives a correct prediction.
% From the programming habits and the statistic of the dataset, the ``property pageY'' is an uncommon terminal only appears in several files but repeats many times in one file. 
% Due to it is uncommon, the basic LSTM model is hard o ltearn enough informatnio and give an incorrect prediction.
% On the contrary, the AT2V-LSTM integration model can leverage the semantic information of ``pageY'' extracted by the ASTToken2Vec embedding and give a correct prediction.



\section{Conclusion}
We proposed an embedding method for AST nodes called ASTToken2Vec. 
We integrated the method with an LSTM model for code completion, which we call AT2V-LSTM.
From the experiments, we manually observed that ASTToken2Vec produces embedding that reflect intuitive similarities of token names and roles.
The application of ASTToken2Vec to the LSTM code completion exhibited slight improvements in prediction accuracy. %These embedding vectors enable our AT2V-LSTM integration model to leverage more semantic knowledge hidden behind ASTs and to complete code with a higher possibility of predicting next tokens correctly. 
% We also extend the way to convert ASTs to sequences in the previous work with two more bits of information so that it is much easier to reconstruct a predicting AST from the sequence.



\bibliographystyle {jssst}
\bibliography {references}



\end{document}
